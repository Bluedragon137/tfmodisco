{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    reload  # Python 2.7\n",
    "except NameError:\n",
    "    try:\n",
    "        from importlib import reload  # Python 3.4+\n",
    "    except ImportError:\n",
    "        from imp import reload  # Python 3.0 - 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.kde module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import modisco\n",
    "\n",
    "onehot_data = np.load('extracted_onehot.npy')[:,:]\n",
    "hypothetical_data = np.load('extracted_hypothetical_scores.npy')[:,:]\n",
    "meannorm_hypothetical_data = (hypothetical_data\n",
    "                              -np.mean(hypothetical_data, axis=-1)[:,:,None])\n",
    "contrib_data = np.load('extracted_contrib_scores.npy')[:,:]\n",
    "#perposimp_hypmeannorm = np.sum(meannorm_hypothetical_data*onehot_data,axis=-1)\n",
    "perposimp = np.sum(contrib_data*onehot_data,axis=-1)\n",
    "\n",
    "flanksize=20\n",
    "contrib_scores_track = modisco.core.DataTrack(name=\"contrib_scores\",\n",
    "                                 fwd_tracks=contrib_data,\n",
    "                                 rev_tracks=contrib_data[:,::-1, ::-1])\n",
    "hypcontrib_scores_track = modisco.core.DataTrack(name=\"hypcontrib_scores\",\n",
    "                                    fwd_tracks=hypothetical_data,\n",
    "                                    rev_tracks=hypothetical_data[:,::-1, ::-1])\n",
    "meannorm_hypcontrib_scores_track = modisco.core.DataTrack(name=\"meannorm_hypcontrib_scores\",\n",
    "                                    fwd_tracks=meannorm_hypothetical_data,\n",
    "                                    rev_tracks=meannorm_hypothetical_data[:,::-1, ::-1])\n",
    "onehot_track = modisco.core.DataTrack(\n",
    "                 name=\"onehot\", fwd_tracks=onehot_data,\n",
    "                 rev_tracks=onehot_data[:,::-1, ::-1])\n",
    "track_set = modisco.core.DataTrackSet(data_tracks=[\n",
    "                contrib_scores_track, hypcontrib_scores_track,\n",
    "                meannorm_hypcontrib_scores_track, onehot_track\n",
    "               ])\n",
    "\n",
    "coords = []\n",
    "for example_idx in range(len(onehot_data)):\n",
    "    #figure out the 6bp window with highest imp\n",
    "    padded_cumsum = np.array([0]+list(np.cumsum(perposimp[example_idx][50:50+41])))\n",
    "    sliding_window_imp = padded_cumsum[7:] - padded_cumsum[:-7]\n",
    "    start = np.argmax(sliding_window_imp)+50\n",
    "    coords.append(modisco.core.Coordinate(\n",
    "                    example_idx=example_idx, start=start-17,\n",
    "                    end=start+24, is_revcomp=False))\n",
    "\n",
    "all_seqlets = [x for x in track_set.create_seqlets(coords=coords, flanks=30)\n",
    "               if np.sum(x[\"contrib_scores\"].corefwd) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seqlets_in_subsample = len(all_seqlets)#40000\n",
    "seqlets_subsample = [all_seqlets[i] for i in np.random.RandomState(1).choice(\n",
    "                     np.arange(len(all_seqlets)), size=n_seqlets_in_subsample,replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98183\n"
     ]
    }
   ],
   "source": [
    "print(n_seqlets_in_subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from deeplift.visualization import viz_sequence\n",
    "\n",
    "corelen=41\n",
    "min_overlap_frac=0.75\n",
    "\n",
    "#define a numba metric to compute similarity\n",
    "@numba.njit(fastmath=True, cache=True)\n",
    "def sliding_window_projected_contin_jacc(x, y):\n",
    "    x_orig_onehot = x[:int(len(x)/2)].reshape((-1, 4))\n",
    "    x_orig_hyp = x[int(len(x)/2):].reshape((-1,4))\n",
    "    y_orig_onehot = y[:int(len(y)/2)].reshape((-1,4))\n",
    "    y_orig_hyp = y[int(len(y)/2):].reshape((-1,4))\n",
    "    \n",
    "    assert x_orig_onehot.shape==y_orig_onehot.shape\n",
    "    assert x_orig_hyp.shape==y_orig_hyp.shape\n",
    "    assert x_orig_onehot.shape==x_orig_hyp.shape\n",
    "    \n",
    "    flanklen = int((len(x_orig_onehot)-corelen)/2)\n",
    "    \n",
    "    min_overlap = int(np.ceil(corelen*min_overlap_frac))\n",
    "    \n",
    "    x_orig_actual = x_orig_hyp*x_orig_onehot\n",
    "    y_orig_actual = y_orig_hyp*y_orig_onehot\n",
    "    \n",
    "    startoffset = -(corelen-min_overlap)\n",
    "    endoffset = (corelen - min_overlap)\n",
    "    assert (corelen-min_overlap) < flanklen\n",
    "    \n",
    "    max_sim = -np.inf\n",
    "    max_imputedx = -np.inf\n",
    "    max_imputedy = -np.inf\n",
    "    for revcomp in [False, True]:\n",
    "        if (revcomp==False):\n",
    "            x_onehot = x_orig_onehot\n",
    "            x_hyp = x_orig_hyp \n",
    "            y_onehot = y_orig_onehot\n",
    "            y_hyp = y_orig_hyp\n",
    "            x_actual = x_orig_actual\n",
    "            y_actual = y_orig_actual\n",
    "        else:\n",
    "            x_onehot = x_orig_onehot\n",
    "            x_hyp = x_orig_hyp\n",
    "            y_onehot = y_orig_onehot[::-1, ::-1]\n",
    "            y_hyp = y_orig_hyp[::-1, ::-1]\n",
    "            x_actual = x_orig_actual\n",
    "            y_actual = y_orig_actual[::-1, ::-1]\n",
    "        \n",
    "        for offset in range(startoffset, endoffset+1):\n",
    "            y_leftpad = max(offset, 0)\n",
    "            y_slicestart = flanklen-y_leftpad\n",
    "            y_rightpad = max(corelen-(corelen+offset),0)\n",
    "            overlap_size = y_leftpad + corelen + y_rightpad\n",
    "\n",
    "            x_leftpad = max(-offset, 0)\n",
    "            x_slicestart = flanklen-x_leftpad\n",
    "            \n",
    "            imputedx_l1norm = 0\n",
    "            imputedy_l1norm = 0\n",
    "            actualx_l1norm = 0\n",
    "            actualy_l1norm = 0\n",
    "            x_actual_list = []\n",
    "            y_actual_list = []\n",
    "            x_imputed_list = []\n",
    "            y_imputed_list = []\n",
    "            for pos_idx in range(overlap_size):\n",
    "                x_pos_idx = x_slicestart + pos_idx\n",
    "                y_pos_idx = y_slicestart + pos_idx\n",
    "                for base_idx in range(4):\n",
    "                    x_actual_here = x_actual[x_pos_idx, base_idx]\n",
    "                    y_actual_here = y_actual[y_pos_idx, base_idx]\n",
    "                    x_imputed_here = x_hyp[x_pos_idx, base_idx]*y_onehot[y_pos_idx, base_idx]\n",
    "                    y_imputed_here = y_hyp[y_pos_idx, base_idx]*x_onehot[x_pos_idx, base_idx]\n",
    "                    \n",
    "                    imputedx_l1norm += abs(x_imputed_here)\n",
    "                    imputedy_l1norm += abs(y_imputed_here)\n",
    "                    actualx_l1norm += abs(x_actual_here)\n",
    "                    actualy_l1norm += abs(y_actual_here)\n",
    "                    \n",
    "                    x_actual_list.append(x_actual_here)\n",
    "                    y_actual_list.append(y_actual_here)\n",
    "                    x_imputed_list.append(x_imputed_here)\n",
    "                    y_imputed_list.append(y_imputed_here)\n",
    "                    \n",
    "            imputedx_union = 0\n",
    "            imputedx_intersection = 0\n",
    "            imputedy_union = 0\n",
    "            imputedy_intersection = 0\n",
    "            for i in range(len(x_actual_list)):\n",
    "                x_actual_here = x_actual_list[i]/actualx_l1norm\n",
    "                y_actual_here = y_actual_list[i]/actualy_l1norm\n",
    "                x_imputed_here = x_imputed_list[i]/imputedx_l1norm\n",
    "                y_imputed_here = y_imputed_list[i]/imputedy_l1norm\n",
    "\n",
    "                abs_x_actual_here = abs(x_actual_here)\n",
    "                abs_y_actual_here = abs(y_actual_here)\n",
    "                abs_x_imputed_here = abs(x_imputed_here)\n",
    "                abs_y_imputed_here = abs(y_imputed_here)\n",
    "\n",
    "                imputedx_union += max(abs_x_imputed_here,abs_y_actual_here)\n",
    "                imputedy_union += max(abs_y_imputed_here,abs_x_actual_here)\n",
    "                imputedx_intersection += (np.sign(x_imputed_here)*np.sign(y_actual_here)\n",
    "                                          *min(abs_x_imputed_here,abs_y_actual_here))\n",
    "                imputedy_intersection += (np.sign(y_imputed_here)*np.sign(x_actual_here)\n",
    "                                          *min(abs_y_imputed_here,abs_x_actual_here))\n",
    "\n",
    "            imputedx_sim = (imputedx_intersection/imputedx_union)\n",
    "            imputedy_sim = (imputedy_intersection/imputedy_union)\n",
    "            sim = 0.5*(imputedx_sim+imputedy_sim)\n",
    "            max_sim = max(max_sim, sim)\n",
    "            #max_imputedx = max(max_imputedx, imputedx_sim)\n",
    "            #max_imputedy = max(max_imputedy, imputedy_sim)\n",
    "    \n",
    "    #print(0.5*(max_imputedx+max_imputedy)) for debugging\n",
    "    #return 1 - 0.5*(max_imputedx+max_imputedy) #for comparison with old calc, for now\n",
    "    return 1-max_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_feature_vectors(seqlets, scoretracknames, flanksize):\n",
    "    feature_vectors = []\n",
    "    for seqlet in seqlets:\n",
    "        tracks = [seqlet[scoretrackname].get_core_with_flank(left=flanksize, right=flanksize,\n",
    "                                                             is_revcomp=False)\n",
    "                  for scoretrackname in scoretracknames]\n",
    "        concatenated_tracks = np.concatenate(tracks, axis=0)\n",
    "        feature_vectors.append(concatenated_tracks.ravel())\n",
    "    return np.array(feature_vectors)\n",
    "\n",
    "featvec = get_feature_vectors(seqlets=seqlets_subsample,\n",
    "                              scoretracknames=[\"onehot\", \"hypcontrib_scores\"],\n",
    "                              flanksize=flanksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  2 14:40:17 2020 Building RP forest with 21 trees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/avanti/anaconda3/envs/py376/lib/python3.7/site-packages/pynndescent/rp_trees.py:747: NumbaTypeSafetyWarning: unsafe cast from UniTuple(int64 x 2) to UniTuple(int32 x 2). Precision may be lost.\n",
      "  leaf_size,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  2 14:40:23 2020 parallel NN descent for 17 iterations\n",
      "\t 0  /  17\n",
      "\t 1  /  17\n",
      "\t 2  /  17\n",
      "\t 3  /  17\n"
     ]
    }
   ],
   "source": [
    "#get a similarity using pynndescent\n",
    "from pynndescent import NNDescent\n",
    "import time\n",
    "n_neighbors = 20\n",
    "start = time.time()\n",
    "nnd = NNDescent(data=featvec,\n",
    "                n_neighbors=n_neighbors,\n",
    "                metric=sliding_window_projected_contin_jacc,\n",
    "                #metric_kwds={'corelen':41, 'min_overlap_frac':0.75},\n",
    "                random_state=1234,\n",
    "                max_candidates=60, #value used in UMAP: https://github.com/lmcinnes/umap/blob/9f66cafdef9c666082b2da188c2ae9bff60bc763/umap/umap_.py#L297\n",
    "                verbose=True,\n",
    "                tree_init=True,\n",
    "                n_jobs=4)\n",
    "knn_indices, knn_dists = nnd.neighbor_graph\n",
    "print(\"Took:\",time.time()-start,\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"allseqlets_pynndescapprox_affmat_recenter_41bpseqlets_0.75.npy\", (knn_indices, knn_dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
